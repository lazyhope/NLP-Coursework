import pandas as pd
import torch
from sklearn.metrics import classification_report
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import AutoTokenizer

from train.train_freezed import PCLClassifier, PCLDataset, train_model


def main():
    # paraphrase data generated by gemini-2.0-flash
    paraphrase_df = pd.read_csv("gemini-flash/positive-paraphrases.csv")
    train_df = pd.read_csv("data/train_data.csv")
    print(
        f"Augmented train data size: {len(train_df)} -> {len(train_df) + len(paraphrase_df)}"
    )
    train_df = pd.concat([train_df, paraphrase_df])
    val_df = pd.read_csv("data/val_data.csv")

    model_name = "roberta-large"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    train_dataset = PCLDataset(
        train_df["text"].values, train_df["label"].values, tokenizer
    )
    val_dataset = PCLDataset(val_df["text"].values, val_df["label"].values, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16)

    # Prepare model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    model = PCLClassifier(model_name)
    model = model.to(device)

    best_model_state, _ = train_model(model, train_loader, val_loader, device)

    model.load_state_dict(best_model_state)
    model.eval()

    test_df = pd.read_csv("data/test_data.csv")
    test_dataset = PCLDataset(
        test_df["text"].values, test_df["label"].values, tokenizer
    )
    test_loader = DataLoader(test_dataset, batch_size=16)

    test_preds = []
    test_labels = []

    progress_bar = tqdm(test_loader, desc="Testing")
    with torch.no_grad():
        for batch in progress_bar:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs, dim=1)

            test_preds.extend(preds.cpu().numpy())
            test_labels.extend(labels.cpu().numpy())

    # Print test results
    test_report = classification_report(test_labels, test_preds)
    print("\nTest Results:")
    print(test_report)


if __name__ == "__main__":
    main()

# Output running on RTX 4080 (To be compared with the output of train/train_freezed.py):
#
# Augmented train data size: 7328 -> 14685
# Using device: cuda
# Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
# Epoch 1/3 [Train]: 100%|████████████████████████████████████| 918/918 [04:24<00:00,  3.48it/s, loss=0.2104]
# Epoch 1/3 [Val]: 100%|████████████████████████████████████████| 66/66 [00:07<00:00,  9.36it/s, loss=0.1047]

# Epoch 1 Summary:
# Train Loss: 0.1670
# Validation Loss: 0.2230
# Validation F1 (Positive Class): 0.5400
# --------------------------------------------------
# New best model saved! F1: 0.5400
# Epoch 2/3 [Train]: 100%|████████████████████████████████████| 918/918 [04:24<00:00,  3.47it/s, loss=0.0337]
# Epoch 2/3 [Val]: 100%|████████████████████████████████████████| 66/66 [00:07<00:00,  9.35it/s, loss=0.0169]

# Epoch 2 Summary:
# Train Loss: 0.0954
# Validation Loss: 0.1975
# Validation F1 (Positive Class): 0.5026
# --------------------------------------------------
# Epoch 3/3 [Train]: 100%|████████████████████████████████████| 918/918 [04:24<00:00,  3.47it/s, loss=0.0025]
# Epoch 3/3 [Val]: 100%|████████████████████████████████████████| 66/66 [00:07<00:00,  9.29it/s, loss=0.0041]

# Epoch 3 Summary:
# Train Loss: 0.0671
# Validation Loss: 0.2517
# Validation F1 (Positive Class): 0.4364
# --------------------------------------------------
# Testing: 100%|███████████████████████████████████████████████████████████| 131/131 [00:14<00:00,  9.28it/s]

# Test Results:
#               precision    recall  f1-score   support

#            0       0.94      0.99      0.96      1894
#            1       0.75      0.41      0.53       199

#     accuracy                           0.93      2093
#    macro avg       0.85      0.70      0.75      2093
# weighted avg       0.92      0.93      0.92      2093
